# Mixture-of-Supernets: Improving Weight-Sharing Supernet Training with Architecture-Routed Mixture-of-Experts

This repository contains the code used in the [Mixture-of-Supernet (MoS) work](https://arxiv.org/abs/2306.04845).

| Folder/File                             | Experiments                                                                                     |
|:---------------------------------------:|:---------------------------------------------------------------------------------------------:|
| [`mos-mt/mos-mt/`](https://github.com/UBC-NLP/MoS/tree/main/mos-mt/mos-mt)                             | Machine Translation                |
| [`mos-bert/mos-bert/`](https://github.com/UBC-NLP/MoS/tree/main/mos-bert/mos-bert)             | BERT Pretraining |

### License
This repository is GPL-licensed.

